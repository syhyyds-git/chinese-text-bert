import os
import re
import json
import random
import requests
import bz2
from tqdm import tqdm
import sys

# --------------------------
# é…ç½®
# --------------------------
WIKI_URL = "https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2"
WIKI_BZ2 = "zhwiki-latest-pages-articles.xml.bz2"
WIKIX_DIR = "wikiextracted"
CORPUS_FILE = "corpus.txt"
TARGET_LINES = 500000  # æœ€ç»ˆ corpus.txt è¡Œæ•°

# --------------------------
# ä¸‹è½½ä¸­æ–‡ç»´åŸº XMLï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
# --------------------------
if not os.path.exists(WIKI_BZ2):
    print(f"ğŸŒ å¼€å§‹ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘ XML æ•°æ®ï¼ˆçº¦ 1~2 GBï¼‰...")
    with requests.get(WIKI_URL, stream=True) as r:
        r.raise_for_status()
        total = int(r.headers.get('content-length', 0))
        with open(WIKI_BZ2, 'wb') as f, tqdm(
            total=total, unit='B', unit_scale=True, desc=WIKI_BZ2
        ) as bar:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
                bar.update(len(chunk))
    print("âœ… ä¸‹è½½å®Œæˆ")
else:
    print(f"âœ… å·²å­˜åœ¨ {WIKI_BZ2}ï¼Œè·³è¿‡ä¸‹è½½")

# --------------------------
# æ£€æŸ¥ WikiExtractor æ˜¯å¦å®‰è£…
# --------------------------
try:
    import wikiextractor
except ImportError:
    print("âš ï¸ WikiExtractor æœªå®‰è£…ï¼Œå¼€å§‹è‡ªåŠ¨å®‰è£…...")
    os.system(f"{sys.executable} -m pip install wikiextractor")

# --------------------------
# ä½¿ç”¨ WikiExtractor æå–æ–‡æœ¬ï¼ˆä»…ç”Ÿæˆå°‘é‡ JSONï¼‰
# --------------------------
if not os.path.exists(WIKIX_DIR):
    os.makedirs(WIKIX_DIR, exist_ok=True)

print("ğŸŒ å¼€å§‹æå–ä¸­æ–‡ç»´åŸºæ–‡æœ¬ï¼ˆå°å‹åŒ–ï¼Œä»…å‰å‡ åƒæ¡ï¼‰...")
# ä½¿ç”¨ Python API æµå¼å¤„ç† bz2 æ–‡ä»¶
def extract_wiki_small(bz2_file, output_dir, max_articles=10000):
    import xml.etree.ElementTree as ET

    def clean_text(text):
        return re.sub(r'\s+', ' ', text).strip()

    count = 0
    with bz2.open(bz2_file, "rt", encoding="utf-8", errors="ignore") as f:
        article_lines = []
        in_page = False
        for line in tqdm(f):
            if "<page>" in line:
                in_page = True
                article_lines = [line]
            elif "</page>" in line:
                article_lines.append(line)
                in_page = False
                xml_str = "".join(article_lines)
                try:
                    root = ET.fromstring(xml_str)
                    title = root.find('title').text
                    text_node = root.find('.//revision/text')
                    if text_node is None or text_node.text is None:
                        continue
                    text = clean_text(text_node.text)
                    # å†™å…¥ JSON
                    file_idx = count // 1000
                    os.makedirs(os.path.join(output_dir, f"{file_idx:03d}"), exist_ok=True)
                    out_path = os.path.join(output_dir, f"{file_idx:03d}", f"{file_idx:03d}_{count%1000:04d}.json")
                    with open(out_path, "w", encoding="utf-8") as fout:
                        json.dump({"title": title, "text": text}, fout, ensure_ascii=False)
                    count += 1
                    if count >= max_articles:
                        return
                except Exception:
                    continue
            elif in_page:
                article_lines.append(line)

extract_wiki_small(WIKI_BZ2, WIKIX_DIR, max_articles=10000)
print(f"âœ… æå–å®Œæˆï¼Œçº¦ {10000} æ¡æ–‡ç« ä¿å­˜åˆ° {WIKIX_DIR}")

# --------------------------
# ç”Ÿæˆ corpus.txt
# --------------------------
print(f"ğŸŒ å¼€å§‹ç”Ÿæˆ {CORPUS_FILE} ...")

all_sentences = []
for root, dirs, files in os.walk(WIKIX_DIR):
    for file in files:
        if not file.endswith(".json"):
            continue
        path = os.path.join(root, file)
        with open(path, "r", encoding="utf-8") as fin:
            data = json.load(fin)
            text = data.get("text", "")
            sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
            for sent in sentences:
                sent = sent.strip()
                if 10 < len(sent) < 200:  # è¿‡æ»¤çŸ­å¥å­
                    all_sentences.append(sent)

# éšæœºæŠ½å– TARGET_LINES æ¡
if len(all_sentences) > TARGET_LINES:
    selected_sentences = random.sample(all_sentences, TARGET_LINES)
else:
    selected_sentences = all_sentences

with open(CORPUS_FILE, "w", encoding="utf-8") as fout:
    for sent in selected_sentences:
        fout.write(sent + "\n")

print(f"âœ… å·²ç”Ÿæˆ {CORPUS_FILE}ï¼Œå…± {len(selected_sentences)} æ¡è¯­æ–™")
